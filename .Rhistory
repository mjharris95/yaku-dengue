# Output: pdf figures in the figs folder if requested and a list with the following entries
#         gsynth_obj: a gsynth object from the fect package
#         att_nums: summary of attributable cases (from  function)
#         years: the years over which climate data is provided
#         year_ind: the indices for when a new year starts
#         cyclone_step: index when cyclone occurred
#         pop: full population size of the anomalous precip districts
#         pop.weights: population weights for the anomalous precip districts (in the order of gsynth_out$tr)
synth_fun <- function(case_df, match_out, pop_df, file_prefix,
att_plot = FALSE,
spatt_plot = FALSE, map = NULL,
big_map = NULL,
lf_num=NA, r.max=5, use_clim=TRUE,
use_r0 = FALSE,
inc=TRUE, start_year = 2010,
exclude_year = c(),
end_week = 52, log_cases=FALSE,
method="ife") {
# fail if requesting a map without providing shapefile
if(spatt_plot==TRUE & is.null(map)){
stop("Provide map to plot")
}
# treated and matched units
incl_units <- c(match_out$treated_names, match_out$match_names)
if(use_clim == TRUE){
# read in climate
df <- fread("clim_df.csv") %>%
mutate(id = str_pad(id, 6, pad="0")) %>%
# filter to treated and matched control regions
filter(id %in% incl_units) %>%
filter(date <= as_date("2023-12-31"))
# set up appropriate lags
df <- left_join(df %>%
select(-rain) %>%
mutate(date = date + 63),
df %>%
select(-c(temp, rel_r0)) %>%
mutate(date = date + 42))
df %<>%
# convert to epiweek
mutate(week=epiweek(date), year=year(date), month=month(date))  %>%
# sometimes the last few days of a year get assigned to first epiweek of following year
mutate(year = ifelse(week == 1 & month == 12, year+1, year)) %>%
group_by(week, year, id) %>%
dplyr::summarize(mean_temp = mean(temp),
mean_rain = mean(rain),
mean_rel_r0 = mean(rel_r0)) %>%
left_join(case_df) %>%
filter(!is.na(mean_temp) & !is.na(year)) %>%
filter(id %in% incl_units) %>%
# filter to specified time period
filter(year >= start_year) %>%
filter(! year %in% exclude_year) %>%
filter(week <= end_week) %>%
filter(year <= 2023)
} else{
# still read in the climate covariates even though we won't use them to make sure the same districts are studied
# read in climate
df <- fread("clim_df.csv") %>%
mutate(id = str_pad(id, 6, pad="0")) %>%
# filter to treated and matched control regions
filter(id %in% incl_units) %>%
filter(date <= as_date("2023-12-31"))
# set up appropriate lags
df <- left_join(df %>%
select(-rain) %>%
mutate(date = date + 63),
df %>%
select(-c(temp, rel_r0)) %>%
mutate(date = date + 42))
df %<>%
# convert to epiweek
mutate(week=epiweek(date), year=year(date), month=month(date))  %>%
# sometimes the last few days of a year get assigned to first epiweek of following year
mutate(year = ifelse(week == 1 & month == 12, year+1, year)) %>%
group_by(week, year, id) %>%
dplyr::summarize(mean_temp = mean(temp),
mean_rain = mean(rain),
mean_rel_r0 = mean(rel_r0)) %>%
left_join(case_df) %>%
filter(!is.na(mean_temp) & !is.na(year)) %>%
# don't actually need the climate covariates
mutate(mean_temp = NA,
mean_rain = NA,
mean_rel_r0 = NA) %>%
filter(id %in% incl_units) %>%
filter(year >= start_year) %>%
filter(! year %in% exclude_year) %>%
filter(week <= end_week) %>%
filter(year <= 2023)
}
# we'll use this to group into 4-week periods
time_df <- df %>%
mutate(time = week+(year-min(case_df$year))*100) %>%
select(time, week, year) %>%
distinct() %>%
arrange(time) %>%
ungroup() %>%
mutate(weeknum = row_number()) %>%
mutate(step = ceiling(weeknum/4)) %>%
mutate(step = as.integer(step))
# will use this in plotting later
year_ind <- sapply(unique(time_df$year), function(x) min(which(time_df$year==x))/4)
years <- unique(time_df$year)
# aggregate to 4-epiweek periods, take average within those
df <- time_df %>%
right_join(df) %>%
group_by(id, step) %>%
dplyr::summarize(cases = sum(cases),
mean_temp = mean(mean_temp),
mean_rel_r0 = mean(mean_rel_r0),
mean_rain = mean(mean_rain)*1000,
year = max(year))
# when did the cyclone happen?
cyclone_step <- unique(time_df$step[time_df$week == 10 & time_df$year==2023])
# input a zero in weeks with missing data
df <- expand.grid(step=1:max(df$step, na.rm=TRUE), id=incl_units) %>%
left_join(df) %>%
left_join(pop_df) %>%
mutate(inc = cases/pop) %>%
mutate(cases = ifelse(is.na(cases), 0, cases),
inc = ifelse(is.na(inc) | is.infinite(inc), 0, inc)) %>%
mutate(int = ifelse(id %in% match_out$treated_names & step >= cyclone_step, 1, 0),
weight = 1) %>%
left_join(., match_out$df %>% select(id, is_control) %>% distinct() %>% filter(is_control %in% c("Anomalous Precipitation", "Matched Control")))
tr_pop <- df %>% filter(int==1 & year==2023) %>% select(pop, id) %>% distinct() %>% filter(!is.na(pop)) %>% select(pop) %>% sum()
#  if specified, use incidence or logged cases as outcome
if(inc == TRUE){
df %<>% mutate(y = inc,
weight = pop)
}
else if(log_cases == TRUE){
df %<>% mutate(y = log(cases+exp(-1)))
}
else{
df %<>% mutate(y = cases)
}
# whether to use cross-validation period or pre-set the number of latent factors
if(is.na(lf_num)){
CV <- TRUE
r <- c(0,r.max)
}
else{
CV <- FALSE
r <- lf_num
}
# lets us get district-level estimates more easily
df$group <- df$id
# running the fect package to get the synthetic control results
if(use_clim == TRUE){
if(use_r0 == TRUE){
gsynth_out <- df %>%
fect(y ~ int + mean_rel_r0, data = .,
index=c("id", "step"),
force = "two-way",
method=method,
se = TRUE,
CV = CV, r = r,
seed = 514,
nboots = 1000,
group="group",
W="weight")
# model without climate covariates
} else {
gsynth_out <- df %>%
fect(y ~ int + mean_temp, data = .,
index=c("id", "step"),
force = "two-way",
method=method,
se = TRUE,
CV = CV, r = r,
seed = 514,
nboots = 1000,
group="group",
W="weight")
}
} else {
gsynth_out <- df %>%
fect(y ~ int, data = .,
index=c("id", "step"),
force = "two-way",
method=method,
se = TRUE,
CV = CV, r = r,
seed = 514,
nboots=1000,
group="group",
W="weight")
}
print("ran")
# get population weights for bootstraps
if(inc == TRUE){
# which treated locations are included in each bootstrap?
boot.locs <- lapply(gsynth_out$boot.ids, function(n) gsynth_out$id[n[1:length(gsynth_out$tr)]])
# what are their population sizes
boot.pops <- lapply(boot.locs, function(boot) unlist(sapply(boot, function(loc) pop_df$pop[which(pop_df$id == loc & pop_df$year == 2023)])))
# population weights for each bootstrap
boot.weights <- lapply(boot.pops, function(each.pop) each.pop/sum(each.pop))
act.locs <- gsynth_out$id[gsynth_out$tr]
act.pops <- sapply(act.locs, function(loc) pop_df$pop[which(pop_df$id == loc & pop_df$year == 2023)])
act.weights <- unlist(act.pops)/tr_pop
}
else{
boot.weights <- lapply(1:length(gsynth_out$boot.ids), function(n) rep(1, length.out = length(gsynth_out$tr)))
}
# calculate percent attributable cases based on bootstraps
lapply(1:length(gsynth_out$Y.boot), function(n) apply(gsynth_out$Y.boot[[n]][,1:length(gsynth_out$tr)], 1, function(x) sum(x*boot.weights[[n]]))) %>%
do.call(cbind, .) %>%
`/`(gsynth_out$att.W.boot, .) %>% ## check this
ifelse(is.infinite(.), 0, .) %>% # set proportion to zero when no reported cases
apply(1, function(x) c(quantile(x, c(.025, .975)), sd(x)/sqrt(length(x)), sum(x<=0)/length(x))) %>% # recalculate 95% CI, standard error, p-value
t() %>%
data.frame() %>%
rename(lower.pct = 1,
upper.pct = 2,
recalc.SE = 3,
recalc.p = 4
) %>%
cbind(obs.inc = apply(gsynth_out$Y.dat[,gsynth_out$tr], 1, function(x) sum(x * act.weights))) %>%
mutate(obs.cases = obs.inc*tr_pop) %>%
mutate(lower.num = lower.pct*obs.inc,  #use reported cases and percent attributable to calculate number attributable
upper.num=upper.pct*obs.inc) %>%
cbind(gsynth_out$est.att.W) %>%
mutate(mid.pct = ATT/obs.inc,
mid.num=ATT,
mid.cases=ATT*tr_pop,
upper.cases=upper.num*tr_pop,
lower.cases=lower.num*tr_pop) -> gsynth_out$est.att
# calculate rsq
gsynth_out$rsq <- cor(c(gsynth_out$Y.dat[1:length(gsynth_out$pre.periods),gsynth_out$tr]),
c(gsynth_out$Y.ct[1:length(gsynth_out$pre.periods),gsynth_out$tr]))^2
# generate plots if requested
if(att_plot==TRUE){
att_plot(df, gsynth_out, cyclone_step, tr_pop, act.weights, file_prefix,
year_ind, unique(time_df$year), inc = inc)
}
if(spatt_plot==TRUE){
spatt_plot(gsynth_out,
map = map, big_map = big_map,
file_prefix=file_prefix)
}
# get attributable numbers
att_nums <- att_print(gsynth_out, df, cyclone_step,
boot.weights, tr_pop)
return(list("gsynth_obj"=gsynth_out,
"att_nums"=att_nums,
"year_ind"=year_ind,
"years"=years,
"cyclone_step"=cyclone_step,
"pop"=tr_pop,
"pop.weights"=act.weights))
}
spatt_plot(synth_out_allper$gsynth_obj, per_map, dept_map, "adm3-allper")
spatt_plot <- function(gsynth_out, map, big_map, file_prefix,
tr_ind=c("2", "3", "4", "5")){
time_ind <- which(rownames(gsynth_out$est.att) %in% tr_ind)
# create a dataframe with columns att (# attributable cases) and id
# summed across period specified by tr_ind in treated districts
att_pc_df <- gsynth_out$eff[time_ind, gsynth_out$tr] %>%
colSums(na.rm=TRUE) %>%
data.frame(att = .,
id = gsynth_out$id[gsynth_out$tr]) %>%
mutate(att_pc = att * 1000) %>%
mutate(att_pc_bin = case_when(att_pc <= -5 ~ "< -5",
att_pc > -5 & att_pc < 0 ~ "-5 to 0",
att_pc > 0 & att_pc <= 5 ~ "0 to 5",
att_pc > 5 & att_pc <= 10 ~ "5 to 10",
att_pc > 10 & att_pc <= 15 ~ "10 to 15",
att_pc > 15 & att_pc <= 20 ~ "15 to 20",
att_pc > 20 & att_pc <= 25 ~ "20 to 25",
att_pc > 25 & att_pc <= 30 ~ "25 to 30",
att_pc > 30 ~ "> 30"))
spatt_map <- merge(map, att_pc_df) %>% # append to map of districts
rename(`Attributable Cases (per Thousand)` = att_pc_bin)  %>%
ggplot()+
geom_sf(aes(fill=`Attributable Cases (per Thousand)`), color="black", lwd=.3)+
geom_sf(data=big_map, fill=NA, color="black", lwd=.8)+ # add big_map borders
scale_fill_manual(values=c("#7E64C5", "#E6CCFF", "#FFCCCC", "#FF9999", "#FF6666", "#FF3333", "#FF0000", "#A10004", "#420008"),
breaks=c("< -5", "-5 to 0", "0 to 5" , "5 to 10", "10 to 15", "15 to 20", "20 to 25", "25 to 30", "> 30"),
labels=c("", "0", "", "10", "", "20", "", ">30", ""))+ #colors specified for main analysis, may wish to modify breaks for a different mode
ylim(c(-9, -3))+
xlim(c(-82, -77))+
theme_void()+
guides(fill=guide_legend(nrow=1,
title.position = "top",
label.position = "bottom"))+
theme(legend.position="bottom")
# begin processing for the time series plots
spatt_df <-  gsynth_out$eff[,gsynth_out$tr] %>%
tail(10) %>% # go from march through the end of 2023
data.frame() %>%
mutate(step = row_number()) # will use this column to convert to dates
spatt_df <-  gsynth_out$eff[,gsynth_out$tr] %>%
tail(10) %>%  # go from march through the end of 2023
data.frame() %>%
mutate(step = row_number()) # will use this column to convert to dates
colnames(spatt_df) <- c(gsynth_out$id[gsynth_out$tr], "step")
# convert to date indices (these will be last date of four-week period across which attributable cases were calculated)
epiweek_df <- data.frame(dates= seq.Date(from = as_date("2023-03-25"),
to = as_date("2023-12-29"),
by = "28 days"),
step=1:10)
# will use this to group districts by region
dept_name <-  read_xlsx("maps/UBIGEODISTRITOS.XLSX") %>%
rename(id = IDDIST,
departamento = NOMBDEP) %>%
select(id, departamento)
# regions that include at least one treated district, ordered to correspond with geographic position when faceted
dept_name$departamento <- factor(dept_name$departamento, levels=c("TUMBES", "CAJAMARCA", "PIURA", "SAN MARTIN", "LAMBAYEQUE", "LA LIBERTAD"))
spatt_scatter <- spatt_df %>%
left_join(epiweek_df) %>%
select(-step) %>%
pivot_longer(!dates, # pivot so we can plot each district as its own line
names_to = "id",
values_to = "TE")  %>%
left_join(dept_name) %>%
mutate(TE_pc = TE * 1000) %>% # calculate attributable cases per thousand people
ggplot(aes(y=TE_pc, x=dates, group=id)) +
geom_line(linewidth=.2, alpha=.5)+
theme_classic()+
facet_wrap(~departamento, ncol=2, scales="free")+ # gives axes to all plots
ylab("Attributable Cases (per Thousand)")+
xlab("Time")+
theme(legend.position="none")+
scale_x_date(date_breaks = "2 months", date_labels = "%b")+ # make the x-axis (time) nice
geom_hline(yintercept=0, color="maroon", linetype="dashed")+ # indicate zero attributable cases
scale_y_continuous(limits=c(-5, 60)) # may need to change this if not plotting main analysis
plot_grid(spatt_scatter)
ggsave(paste0("figs/", file_prefix, "-spatt-scatter.pdf"), height=6, width=8, units="in", dpi=700)
plot_grid(spatt_map, spatt_scatter, ncol = 2, labels="AUTO") # combine both plots into a grid
ggsave(paste0("figs/", file_prefix, "-spatt-map.pdf"), height=6, width=8, units="in", dpi=700)
}
spatt_plot(synth_out_allper$gsynth_obj, per_map, dept_map, "adm3-allper")
# the cov directory contains .tif files of different vulnerability indices - excluded from
# public respository for privacy reasons
files <- dir("cov")
files
covar_df
load("covar_df.RData")
covar_df
#### ANALYSIS FOR PART 2: SOCIOVULNERABILITY INDICES BEGINS HERE
# would recommend detaching other packages due to interference between dplyr and terra select
library(raster)
library(terra)
library(psych)
# read in shapefile of districts
per_map<-read_sf("maps/CDC_Distritos.shp") %>%
rename(id = ubigeo)
# the cov directory contains .tif files of different vulnerability indices - excluded from
# public respository for privacy reasons
files <- dir("cov")
#exclude population density
files <- files[!startsWith(files, "r_dpob")]
covar_df <- c()
# loop through covariate files
for(file in files[startsWith(files, "r_hidro")]){
# identify the department based on file name
dept <-  file %>%
str_split(., pattern=".tif") %>%
unlist() %>%
head(1) %>%
str_split(., pattern="_") %>%
unlist() %>%
tail(1)
# issue with string splitting because LA LIBERTAD is two words
dept <- ifelse(dept == "LIBERTAD", "LA LIBERTAD", dept)
dept <- ifelse(dept == "MARTIN", "SAN MARTIN", dept)
# read in corresponding population density file
dpob <-  switch(dept,
"TUMBES" = "r_dpob_TUMBES_050.tif",
"LA LIBERTAD" = "r_dpob_LA_LIBERTAD_050.tif",
"LAMBAYEQUE" = "r_dpob_LAMBAYEQUE_050.tif",
"PIURA" = "r_dpob_PIURA_050.tif",
"CAJAMARCA" = "r_dpob_CAJAMARCA_050.tif",
"SAN MARTIN" = "r_dpob_SAN_MARTIN_050.tif" ) %>%
paste0("cov/", .) %>%
rast()
# what is the variable name?
var_name <- file %>%
str_split(., "r_") %>%
unlist() %>%
tail(-1) %>%
paste("collapse" = "r_") %>%
str_split(., "_[:upper:]")  %>%
unlist() %>%
head(1)
# read in the covariate raster
socio <- rast(paste0("cov/" , file))
# align the rasters
dpob <- project(dpob, per_map)
socio <- project(socio, dpob)
dpob <- crop(dpob, socio)
# weight covariates based on population
socio_mult <- socio * dpob
# get population-weighted sum
socio_sum <- terra::extract(socio_mult, per_map,
fun="sum", na.rm=TRUE,
bind = TRUE) %>%
st_drop_geometry() %>%
data.frame() %>%
rename(var = 5)
# get population and calculate pop-weighted mean
covar_df <- terra::extract(dpob, per_map,
fun="sum", na.rm=TRUE,
bind = TRUE) %>%
st_drop_geometry %>%
data.frame() %>%
rename(pop = 5) %>%
left_join(socio_sum) %>%
mutate(avg_socio = var/pop) %>%
mutate(var_name = var_name) %>%
filter(DEPARTAMEN == dept) %>%
rbind(covar_df)
# this loop takes a while to run, save each step
save(covar_df, file="covar_df.RData")
}
save(covar_df, "covar_df.RData")
save(covar_df, file="covar_df.RData")
# load in the matching and gsynth results
load("peru-main.RData")
# filter covariates to the anomalous precip districts
covar_df %<>%
filter(id %in% match_out_allper$treated_names)
set.seed(0514)
# pivot wider - format for pca package
covar_df %<>%
distinct() %>%
dplyr::select(c(id, avg_socio, var_name)) %>%
pivot_wider(names_from=var_name, values_from=avg_socio) %>%
distinct()
# append weather information during Cyclone Yaku
covar_df <- read.csv("anomaly_df.csv") %>%
dplyr::select(id, temp, rain) %>%
right_join(covar_df) %>%
filter(!is.nan(vui))
#### ANALYSIS FOR PART 2: SOCIOVULNERABILITY INDICES BEGINS HERE
# would recommend detaching other packages due to interference between dplyr and terra select
library(raster)
library(terra)
library(psych)
# read in shapefile of districts
per_map<-read_sf("maps/CDC_Distritos.shp") %>%
rename(id = ubigeo)
# the cov directory contains .tif files of different vulnerability indices - excluded from
# public respository for privacy reasons
files <- dir("cov")
#exclude population density
files <- files[!startsWith(files, "r_dpob")]
covar_df <- c()
# loop through covariate files
for(file in files[startsWith(files, "r_")]){
# identify the department based on file name
dept <-  file %>%
str_split(., pattern=".tif") %>%
unlist() %>%
head(1) %>%
str_split(., pattern="_") %>%
unlist() %>%
tail(1)
# issue with string splitting because LA LIBERTAD is two words
dept <- ifelse(dept == "LIBERTAD", "LA LIBERTAD", dept)
dept <- ifelse(dept == "MARTIN", "SAN MARTIN", dept)
# read in corresponding population density file
dpob <-  switch(dept,
"TUMBES" = "r_dpob_TUMBES_050.tif",
"LA LIBERTAD" = "r_dpob_LA_LIBERTAD_050.tif",
"LAMBAYEQUE" = "r_dpob_LAMBAYEQUE_050.tif",
"PIURA" = "r_dpob_PIURA_050.tif",
"CAJAMARCA" = "r_dpob_CAJAMARCA_050.tif",
"SAN MARTIN" = "r_dpob_SAN_MARTIN_050.tif" ) %>%
paste0("cov/", .) %>%
rast()
# what is the variable name?
var_name <- file %>%
str_split(., "r_") %>%
unlist() %>%
tail(-1) %>%
paste("collapse" = "r_") %>%
str_split(., "_[:upper:]")  %>%
unlist() %>%
head(1)
# read in the covariate raster
socio <- rast(paste0("cov/" , file))
# align the rasters
dpob <- project(dpob, per_map)
socio <- project(socio, dpob)
dpob <- crop(dpob, socio)
# weight covariates based on population
socio_mult <- socio * dpob
# get population-weighted sum
socio_sum <- terra::extract(socio_mult, per_map,
fun="sum", na.rm=TRUE,
bind = TRUE) %>%
st_drop_geometry() %>%
data.frame() %>%
rename(var = 5)
# get population and calculate pop-weighted mean
covar_df <- terra::extract(dpob, per_map,
fun="sum", na.rm=TRUE,
bind = TRUE) %>%
st_drop_geometry %>%
data.frame() %>%
rename(pop = 5) %>%
left_join(socio_sum) %>%
mutate(avg_socio = var/pop) %>%
mutate(var_name = var_name) %>%
filter(DEPARTAMEN == dept) %>%
rbind(covar_df)
# this loop takes a while to run, save each step
save(covar_df, file="covar_df.RData")
}
load("covar_df.RData")
